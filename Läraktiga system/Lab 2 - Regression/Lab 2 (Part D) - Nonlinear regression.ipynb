{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 (Part D) - Nonlinear regression\n",
    "\n",
    "Make sure that you check the videos of lecture 2 before starting this Lab:\n",
    "- Introduction to Linear Regression: https://youtu.be/-wmjwMWRsZU\n",
    "- Introduction to Nonlinear Regression: https://youtu.be/Hyu8QMLEHrE\n",
    "\n",
    "In this part, you will implement a nonlinear kernel regression. We will use the same house pricing dataset as previously. The following code simply loads the dataset from the data file into the variables $X$ and $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization, always run this cell before anything else\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "filename = \"datasets/ex1data2.txt\"\n",
    "mydata = np.genfromtxt(filename, delimiter=\",\")\n",
    "\n",
    "# We have n data-points\n",
    "n = len(mydata)\n",
    "\n",
    "# X is a matrix of two column, i.e. an array of n 2-dimensional data-points\n",
    "X = mydata[:, :2].reshape(n, 2)\n",
    "\n",
    "# y is the vector of outputs, i.e. an array of n scalar values\n",
    "y = mydata[:, -1]\n",
    "\n",
    "\"\"\" TODO:\n",
    "print a subset of X and y to see how it looks like\n",
    "\"\"\"\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are asked to implement the *Nadaraya-Watson estimator*, which consists in a kernel regression using the *Gaussian kernel* function.\n",
    "\n",
    "The Gaussian kernel function of two vectors $v$ and $u$ is defined as $k(u, v) = e^{{-\\left \\| u - v \\right \\|}^2 ~/~ 2 \\sigma^2}$, where $\\sigma$ (sigma) is a hyperparameter representing the width of the Gaussian. The equation of the Gaussian kernel can be simplified to $k(u, v) = e^{{- \\gamma ~ \\left \\| u - v \\right \\|}^2}$ by considering a hyperparameter gamma: $\\gamma = \\frac{1}{\\sigma^2}$. Complete the code below to write the Gaussian kernel function `gaussian_kernel(..)` corresponding to $k(u, v) = e^{{- \\gamma ~ \\left \\| u - v \\right \\|}^2}$. You can use the Python function `math.exp(..)` to compute the exponancial, and `np.linalg.norm(u - v)` (or your own function) to compute the distance belween two vectors (numpy arrays) $u$ and $v$.\n",
    "\n",
    "The hypothesis function $h(x)$ to make a prediction about a new test data-point $x$ (i.e. predict the price of a new house), is defined as follows:\n",
    "$$h(x) = \\frac{1}{\\sum_{i=1}^n k(x, x^{(i)})} \\sum_{i=1}^n k(x, x^{(i)}) ~ y^{(i)},$$\n",
    "where $x$ is the test data-point, $x^{(i)}$ is the $i^{th}$ training data-point, and $y^{(i)}$ is the output (price) corresponding to the $i^{th}$ training data-point. Complete the code below to write `h(..)` the hypothesis function (which calls the `gaussian_kernel(..)` function).\n",
    "\n",
    "Once the hypothesis function `h(..)` is implemented, use it with $\\gamma = 0.00005$ to make a price prediction for a new house of 1650-square-foot with 3 bedrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" TODO: \n",
    "Write the definition of the gaussian_kernel(...) function. It takes as \n",
    "arguments two vectors u and v, and a hyperparameter gamma. This function \n",
    "can be considered as a measure of similarity between u and v.\n",
    "\"\"\"\n",
    "def gaussian_kernel(u, v, gamma):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "\"\"\" TODO: \n",
    "Write the code for the hypothesis function h. The arguments are:\n",
    "*** x: one new test data-point\n",
    "*** X: the training data\n",
    "*** y: the training outputs\n",
    "*** gamma: the hyperparameter of the gaussian kernel\n",
    "Note: be careful about devisions by zero. Hint: if the divisor is 0, then you can try \n",
    "re-calling the same function with a smaller gamma, e.g: return h(x, X, y, gamma*0.1)\n",
    "\"\"\"\n",
    "def h(x, X, y, gamma):\n",
    "    # An array containing the similarity between x and all the others data-points in X :\n",
    "    similarities = np.array([ gaussian_kernel(x, xi, gamma) for xi in X ])\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Make a prediction for a new house x of 1650-square-foot with 3 bedrooms using gamma = 0.00005\n",
    "\"\"\"\n",
    "gamma = 0.00005\n",
    "x = np.array([1650, 3])\n",
    "# prediction = ...\n",
    "# print(\"The prediction on x is:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see that the hyperparameter $\\gamma$ affects the predicted output, you are asked to vary $\\gamma$ between $1e-10$ and $10e-5$ (you can use `np.arange(1e-10, 10e-5, 1e-5)` in Python), and see how the price predicted for the house `x = np.array([1650, 3])` varies with $\\gamma$. Do a plot of the $\\gamma$ values with respect to the price predicted for house `x`.\n",
    "\n",
    "**Note**: We will see later in the course how to automatically select a good value for hyperparameters such as $\\gamma$, using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "\"\"\" TODO:\n",
    "Complete the following code to predict the price of x based on different values of gamma\n",
    "\"\"\"\n",
    "\n",
    "x = np.array([1650, 3])\n",
    "gammas_list = np.arange(1e-10, 10e-5, 1e-5)\n",
    "predictions = []\n",
    "\n",
    "for gamma in gammas_list:\n",
    "    pass\n",
    "    # prediction = ... make a prediction on x using the current gamma ...\n",
    "    # append the prediction to the list of predictions\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# TODO: plot the values the predicted prices of x with respect to the corresponding values of gamma\n",
    "# ...\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split the training dataset into two parts. The first part `X_train` (and `y_train`) is used as a training data. The second part `X_test` is used to make price predictions and compare it to the true prices `y_test`.\n",
    "\n",
    "Complete the code to produce a list `y_pred` which contains the predicted price of each house in `X_test`. Then, produce a scatter plot based on `X_test`, `y_pred` and `y_test`, which shows the first feature (house size) on one axis vs. the predicted and true price on the other axis. The plot should look like the Figure below.\n",
    "\n",
    "Try several values of $\\gamma$ (e.g. $0.5, 0.00005, 0.00000001$) and see each time on the plot how your predicted prices change. Notice that when $\\gamma$ is smaller, the predicted values tend to be similar regradless of the test houses. Why ? Think about it before looking at the answer below.\n",
    "\n",
    "*Answer*: When $\\gamma$ is set to a very small value, the Gaussian width $\\sigma$ is very large. This makes all the values $\\{ k(x, x^{(i)}) \\mid i=1 \\dots n \\}$ large (close to 1), i.e., all the training data-points will equaly influence $h(x)$  (the predicted price of $x$). Therefore, the predicted price of any test point $x$ will just be the average price of all training houses.\n",
    "\n",
    "<img src=\"imgs/scatterLab2D.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.00005 # 0.5, 0.00005, 0.00000001\n",
    "n = len(X)\n",
    "\n",
    "# The training part:\n",
    "X_train = X[ : n//2] # First half of the dataset\n",
    "y_train = y[ : n//2] # Outputs corresponding to the first half\n",
    "\n",
    "# The testing part:\n",
    "X_test = X[n//2 : ] # Second half of the dataset\n",
    "y_test = y[n//2 : ] # Outputs corresponding to the second half\n",
    "\n",
    "\n",
    "\"\"\" TODO: \n",
    "Based on X_train and y_train, predict the price of each house in X_test. \n",
    "These predictions will be in a list named y_pred.\n",
    "\"\"\"\n",
    "# y_pred = ...\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Complete the following code to produce a figure similar to the one shown above.\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots()\n",
    "# TODO: scatter plot of the first feature (column 0) of X_test vs. y_test\n",
    "# TODO: scatter plot of the first feature (column 0) of X_test vs. y_pred\n",
    "# ...\n",
    "# ...\n",
    "# ...\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear regression with scikit-learn (sklearn)\n",
    "Now you will see an example of nonlinear regression with scikit-learn.\n",
    "\n",
    "First, let's randomly split our dataset into training and testing parts. The test dataset will later serve as a way to evaluate how well our trained model will perform on new unseen data (when deployed in real-world). Splitting is easy to do in scikit-learn using the `train_test_split` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Kepp 30% of the data for test (i.e. test_size=0.3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(\"Full data:\", X.shape, y.shape)\n",
    "print(\"Train data:\", X_train.shape, y_train.shape)\n",
    "print(\"Test data:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the following link for more information about the `train_test_split` function: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Now let's fit a `StandardScaler` to our data in order to scale it before training (as we did in a previous part of the Lab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_normalized = scaler.transform(X_train) # Scale the training data\n",
    "X_test_normalized = scaler.transform(X_test) # Scale the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a nonlinear regression model called `KernelRidge`, then apply it on our test data, and compute the mean squared error. For more information about `KernelRidge`, check: https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = KernelRidge(kernel='rbf', gamma=0.05).fit(X_train_normalized, y_train) # Training\n",
    "y_pred = reg.predict(X_test_normalized) # Predicting\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred) # Same as: ((y_test - y_pred)**2).mean()\n",
    "print(\"The MSE on the test data is:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we used some hyper-parameters chosen manually when training `KernelRidge`. We used a Gaussian kernel (also called *radial basis function* or `rbf`) with `gamma=0.05` set to $0.05$. Later in the course, we will see how to fine-tune the values of such hyper-parameters automatically using *cross validation*. For mor information, you can already check: https://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "\n",
    "You can also try other nonlinear regression methods that we saw in the lectures, such as the k-nearest-neighbours regression:\n",
    "- https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-regression\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
