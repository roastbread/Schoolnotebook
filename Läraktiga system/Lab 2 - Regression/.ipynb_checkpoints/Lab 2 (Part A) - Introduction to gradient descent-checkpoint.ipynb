{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Lab 2 (Part A) - Introduction to gradient descent\n",
    "\n",
    "This part of the Lab is a step by step introduction to the gradient descent algorithm. It will help you understand how it works. Make sure that you check the videos of lecture 2 before starting this Lab:\n",
    "- Introduction to Linear Regression: https://www.youtube.com/watch?v=-wmjwMWRsZU&list=PLS8J_PRPtGfdnPf9QqT7Itnn2rAHsoWqY&index=3\n",
    "- Introduction to Nonlinear Regression: https://www.youtube.com/watch?v=Hyu8QMLEHrE&list=PLS8J_PRPtGfdnPf9QqT7Itnn2rAHsoWqY&index=4\n",
    "\n",
    "First, please select the Python code cell below and run it to initialize some plots. You DO NOT have to understand the code in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Always run this cell before anything else. DO NOT modify this code.\n",
    "%matplotlib notebook\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'labutils/')\n",
    "\n",
    "from lab2utils import lab2partA1, lab2partA2\n",
    "lab2A1, lab2A2 = lab2partA1(), lab2partA2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 1. Minimizing a function of one parameter with gradient descent\n",
    "\n",
    "In this section, you are given a function $F(a) = (a + 5)^2$ of one parameter $a$ (a scalar value). The goal is to minimize the function $F$ using the gradient descent algorithm.\n",
    "\n",
    "You are asked to read and complete the Python code below to perform gradient descent (read carefully the code, the comments, and the *TODO* comments in red). The function `dF(a)` that you should complete corresponds to $\\frac{\\partial F(a)}{\\partial a}$, i.e., the derivative of the function $F(a)$ with respect to the parameter $a$.\n",
    "\n",
    "If your implementation of gradient descent is correct and the value of $\\alpha$ is correctly choosen, then the value of $a$ should approach $-5$ and the value of $F(a)$ should approach $0$. This is because the minimum of the function $F(a)$ is $0$ when $a = -5$.\n",
    "\n",
    "Once your code works well, you can re-run it with different values of the learning rate $\\alpha$ and see the difference in terms of the number of iterations it takes until convergence. For example, you can try the following values for $\\alpha$: 0.01, 0.3, and 0.9. Please note that if your learning rate $\\alpha$ is too large (e.g. $\\alpha = 1.5$ for this example), then $F(a)$ can **diverge** and *blow up*, resulting in values which are too large for computer calculations. If your value of $F(a)$ increases or even blows up, stop the execution, adjust your learning rate and try again.\n",
    "\n",
    "You can also re-run the code with a different initial value for the parameter $a$. For example, you can try an initial value of $a = 0$ or $a = -15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DO NOT modify the definition of the function F(a)\n",
    "def F(a):\n",
    "    return (a + 5)**2\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Write here the definition of the function `dF(a)`, which is\n",
    "the derivative of `F(a)` with respect to the parameter `a`\n",
    "\"\"\"\n",
    "def dF(a):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "alpha = 0.1            # The learning rate of gradient descent\n",
    "a = 7                  # The initial value of a (any initial value is ok)\n",
    "max_iterations = 100   # Maximum number of iterations to perform\n",
    "epsilon = 0.0001       # Some small number to test for convergence (i.e. to stop if F(a) does not decrease too much)\n",
    "\n",
    "for itr in range(max_iterations):\n",
    "    lab2A1.plot(itr, F, a) # This plots an animation (DO NOT modify this line)\n",
    "    prev = F(a) # Save the value of F(a)\n",
    "    \n",
    "    \n",
    "    \"\"\" TODO:\n",
    "    Write here the gradient descent step to update the value  of the parameter a.\n",
    "    Hint: You need to use `alpha` and `dF(a)`.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    \"\"\" TODO:\n",
    "    Replace the boolean variable `CONDITION` below with a condition to break-out\n",
    "    of the loop if we are close to convergence. Hint: You need to use `prev` (the \n",
    "    previous value of F(a)), the current value of `F(a)` (after a has been updated), and `epsilon`.\n",
    "    \"\"\"\n",
    "    CONDITION = True # Replace True with a boolean condition\n",
    "    if CONDITION:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 2. Minimizing a function of two parameters with gradient descent\n",
    "\n",
    "In this section, you are given a function $F(a, b) = 5 + a^2 + \\frac{3}{2} b^2 + a b~$ of two parameters $a$ and $b$ (scalar values). The goal is to minimize the function $F(a, b)$ using the gradient descent algorithm.\n",
    "\n",
    "You are asked to read and complete the Python code below to perform gradient descent (read carefully the code, the comments, and the *TODO* comments in red). The first function `dFa(a, b)` that you should complete corresponds to $\\frac{\\partial F(a, b)}{\\partial a}$, i.e., the derivative of the function $F(a, b)$ with respect to the first parameter $a$. The second function `dFb(a, b)` that you should complete corresponds to $\\frac{\\partial F(a, b)}{\\partial b}$, i.e., the derivative of the function $F(a, b)$ with respect to the second parameter $b$.\n",
    "\n",
    "Note that $\\frac{\\partial F(a, b)}{\\partial a} = 2 a + b$, and $\\frac{\\partial F(a, b)}{\\partial b} = 3 b + a$.\n",
    "\n",
    "If your implementation is correct and the value of $\\alpha$ is correctly chosen, then the value of both $a$ and $b$ should approach $0$ and the value of $F(a, b)$ should approach $5$. This is because the minimum of the function $F(a, b)$ is $5$ when $a = 0$ and $b = 0$.\n",
    "\n",
    "Once your code works well, you can re-run it with different values of the learning rate $\\alpha$, and different values of the initial parameters $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DO NOT modify the definition of the function F(a, b)\n",
    "def F(a, b):\n",
    "    return 5 + a**2 + 1.5 * b**2 + a * b\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Write here the definition of the function dFa(a, b), which is\n",
    "the derivative of F(a, b) with respect to the first parameter a\n",
    "\"\"\"\n",
    "def dFa(a, b):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Write here the definition of the function dFb(a, b), which is\n",
    "the derivative of F(a, b) with respect to the second parameter b\n",
    "\"\"\"\n",
    "def dFb(a, b):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "alpha = 0.1            # The learning rate of gradient descent\n",
    "a, b = 80, 90          # The initial values of a and b (any initial values are ok)\n",
    "max_iterations = 100   # Maximum number of iterations\n",
    "epsilon = 0.0001       # Some small number to test for convergence (i.e. to stop if F(a) does not decrease too much)\n",
    "\n",
    "for itr in range(max_iterations):\n",
    "    lab2A2.plot(itr, F, a, b) # This plots an animation (DO NOT modify this line)\n",
    "    prev = F(a, b) # Save the value of F(a, b)\n",
    "    \n",
    "    \n",
    "    \"\"\" TODO:\n",
    "    Write here the gradient descent step to update the value of parameters `a` and `b` simultaneously.\n",
    "    Hint: You need to use `alpha`, `dFa(a, b)` and `dFb(a, b)`\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    \"\"\" TODO:\n",
    "    Replace the boolean variable `CONDITION` below with a condition to break-out\n",
    "    of the loop if we are close to convergence. You need to use `prev` the previous value of\n",
    "    F(a, b), the current value of `F(a, b)`, and `epsilon`.\n",
    "    \"\"\"\n",
    "    CONDITION = True # Replace True with a boolean condition\n",
    "    if CONDITION:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# 3. Minimizing a function of multiple parameters with gradient descent\n",
    "\n",
    "This section is similar to the previous one, but you will minimize a function of multiple parameters (i.e., a vector of $p$ parameters: $\\theta \\in \\mathbb{R}^p$).\n",
    "\n",
    "First, you are asked to write the function $F(\\theta)$ in the following Python code. The function $F(\\theta)$ is defined as:\n",
    "$$F(\\theta) = \\sum_{j} \\theta_j^2$$\n",
    "\n",
    "The gradient of the function $F(\\theta)$ is denoted as $\\nabla F(\\theta)$. This is a vector containing the derivative of $F(\\theta)$ with respect to each parameter $\\theta_j$:\n",
    "$$\\nabla F(\\theta) = \\left ( \\frac{\\partial F(\\theta)}{\\partial \\theta_0}, \\frac{\\partial F(\\theta)}{\\partial \\theta_1}, \\frac{\\partial F(\\theta)}{\\partial \\theta_2}, \\dots \\right )$$ \n",
    "\n",
    "Write the definition of the function `gradF(theta)` in the following Python code. This function corresponds to $\\nabla F(\\theta)$. It should return an array containing the derivative of $F(\\theta)$ with respect to each parameter $\\theta_j$.\n",
    "\n",
    "If your implementation is correct and the value of $\\alpha$ is correctly chosen, then you should end up getting all parameter values close to $0$ and the value of $F(\\theta)$ should approach $0$. This is because the minimum of the function $F(\\theta)$ is $0$ when $\\theta = \\vec{0} $ (the null vector).\n",
    "\n",
    "Once your code works well, you can re-run it with a different value of the learning rate $\\alpha$, and a different initial parameters vector $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np, matplotlib.pylab as plt\n",
    "\n",
    "\"\"\" TODO:\n",
    "Write here the definition of the function `F(theta)`, where `theta` is an array of parameters.\n",
    "\"\"\"\n",
    "def F(theta):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Write here the definition of the function `gradF(theta)`, the gradient of F(theta).\n",
    "This function should return an array where the j'th value is this array is the \n",
    "derivative of F(theta) with respect to the j'th parameter theta_j.\n",
    "\"\"\"\n",
    "def gradF(theta):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "alpha = 0.1                      # The learning rate of gradient descent\n",
    "theta = np.array([80, 90, -20])  # Some initial parameters vector: theta = [theta_0, theta_1, theta_2, ...]\n",
    "max_iterations = 100             # Maximum number of iterations\n",
    "epsilon = 0.000001               # Some small number to test for convergence (i.e. to stop if F(a) does not decrease too much)\n",
    "\n",
    "for itr in range(max_iterations):\n",
    "    prev = F(theta)\n",
    "    print(\"iteration = {}, theta = {}, F(theta) = {}\".format(itr, theta, prev))\n",
    "    \n",
    "    \"\"\" TODO:\n",
    "    Write here the gradient descent step to update the parameters vector `theta`.\n",
    "    All the parameter values in theta should be updated simultaneously.\n",
    "    Hint: You need to use `alpha` and `gradF(theta)`\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    \"\"\" TODO:\n",
    "    Replace the boolean variable `CONDITION` below with a condition to break-out\n",
    "    of the loop if we are close to convergence. Hint: You need to use `prev` the \n",
    "    previous value of F(theta), the current value of `F(theta)`, and `epsilon`.\n",
    "    \"\"\"\n",
    "    CONDITION = True # Replace True with a boolean condition\n",
    "    if CONDITION:\n",
    "        break\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Produce a plot here showing the value of F(theta) at each iteration.\n",
    "You might need to modify the above code to save all the historical values of F(theta)\n",
    "Note: you can use ax.plot(...) to do this plot\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots()\n",
    "# Plot here the number of iterations vs. the history of values of F(theta)\n",
    "ax.set_xlabel(\"Number of iterations\")\n",
    "ax.set_ylabel(\"F(theta)\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
