{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: K-means Clustering\n",
    "\n",
    "In this Lab, you will implement the K-means clustering algorithm and apply it to compress an image.\n",
    "\n",
    "You will first start on an example 2D dataset that will help you gain an intuition of how the K-means algorithm works. After that, you wil use the K-means algorithm for image compression by reducing the number of colors that occur in an image to only those that are most common in that image.\n",
    "\n",
    "## 1. Implementing K-means\n",
    "The K-means algorithm is a method to automatically cluster similar data examples together. Concretely, you are given a training set $\\{ x^{(1)}, \\dots, x^{(n)} \\}$ (where $x^{(i)} \\in \\mathbb{R}^d$), and want to group the data into a few cohesive \"*clusters*\". The intuition behind K-means is an iterative procedure that starts by guessing the initial centroids, and then refines this guess by repeatedly assigning examples to their closest centroids and then recomputing the centroids based on the assignments.\n",
    "\n",
    "The K-means algorithm is as follows:\n",
    "```python\n",
    "# Initialize centroids\n",
    "centroids = kMeansInitCentroids(X, K)\n",
    "for itr in range(0, iterations):\n",
    "    # \"Cluster assignment\" step: Assign each data point to the closest centroid. \n",
    "    # idx[i] corresponds to the index of the centroid assigned to data-point i\n",
    "    idx = findClosestCentroids(X, centroids)\n",
    "    \n",
    "    # \"Move centroid\" step: Compute means based on centroid assignments\n",
    "    centroids = computeCentroids(X, idx, K)\n",
    "```\n",
    "\n",
    "The inner-loop of the algorithm repeatedly carries out two steps: (i) Assigning each training example $x^{(i)}$ to its closest centroid, and (ii) Recomputing the mean of each centroid using the points assigned to it. The K-means algorithm will always converge to some final set of means for the centroids. Note that the converged solution may not always be ideal and depends on the initial setting of the centroids. Therefore, in practice the K-means algorithm is usually run a few times with different random initializations.\n",
    "\n",
    "You will implement the two phases of the K-means algorithm separately in the next sections.\n",
    "\n",
    "### 1.1. Finding closest centroids\n",
    "In the \"cluster assignment\" phase of the K-means algorithm, the algorithm assigns every training example $x^{(i)}$ to its closest centroid, given the current positions of centroids. Specifically, for every example $i$ we set\n",
    "$$c^{(i)} = j \\text{ that minimizes } \\left \\| x^{(i)} - \\mu_j \\right \\|^2$$\n",
    "where $c^{(i)}$ is the index of the centroid that is closest to $x^{(i)}$, and $\\mu_j \\in \\mathbb{R}^d$ is the position (vector of values) of the $j^{th}$ centroid. Note that $c^{(i)}$ corresponds to `idx[i]` in the algorithm shown above.\n",
    "\n",
    "Your task is to complete the function `findClosestCentroids(X, centroids)` in the following Python code. This function takes the data matrix $X$ and the centroids inside `centroids` and should output a one-dimensional array `idx` that holds the index (a value in $\\{1, \\dots, K\\}$ where $K$ is total number of centroids) of the closest centroid to every training example. The length of the array `idx` should be the same as the number of data-points (i.e. `len(idx) == len(X) == n`). You can implement this using a loop over every training example and\n",
    "every centroid.\n",
    "\n",
    "Once you have completed the function `findClosestCentroids(..)`, you can test it using the examples `X` and `centroids` provided in the code below. If you implemented the function correctly, you should get the array `[1  2  0  0]` (i.e. we have $K=3$ centroinds; the data-point `X[0]` is assigned to cluster centroid `1`, the data-point `X[1]` is assigned to cluster centroid `2`, the data-point `X[2]` is assigned to cluster centroid `0`, and the data-point `X[3]` is assigned to cluster centroid `0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Complete the definition of the function findClosestCentroids(X, centroids). This \n",
    "function takes the data matrix X and the centroids, and should output an array \n",
    "idx that holds the index of the closest centroid to every training data-point.\n",
    "\"\"\"\n",
    "def findClosestCentroids(X, centroids):\n",
    "    # The idx list will contain the index of the closest centroid to each data-point\n",
    "    idx = []\n",
    "    \n",
    "    # For each data-point xi from our dataset X\n",
    "    for xi in X:\n",
    "        # TODO: compute the Euclidean distance from x to all centoids. The results should be in a list distances\n",
    "        distances = [np.linalg.norm(xi- centroid)**2 for centroid in centroids]\n",
    "        # TODO: find the index j corresponding to the smallest distance in distances (you can use np.argmin(..))\n",
    "        j = np.argmin(distances)\n",
    "        \n",
    "        # TODO: append the index of the closest centroid from xi, to the list idx\n",
    "        idx.append(j)\n",
    "    \n",
    "    # Return the list idx as an array\n",
    "    return np.array(idx)\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Test your function findClosestCentroids(..) by calling it using \n",
    "the examples X and centroids given below. If you implemented the \n",
    "function correctly, you should get [1, 2, 0, 0]\n",
    "\"\"\"\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [9, 11]]) # Example dataset with 4 data-points\n",
    "centroids = np.array([[7, 5], [0, 2], [3, 3]])  # Initial centroids (we have K=3 centroids)\n",
    "idx = findClosestCentroids(X, centroids)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Computing centroid means\n",
    "Given assignments of every point to a centroid, the second phase of the algorithm recomputes, for each centroid, the mean of the points that were assigned to it. Specifically, for every centroid $j$ we set it to:\n",
    "$$\\mu_j = \\frac{1}{|C_j|} \\sum_{i \\in C_j} x^{(i)}$$\n",
    "where $C_j$ is the set of examples that are assigned to centroid $j$ (i.e. the $j^{th}$ cluster). Concretely, if two examples say $x^{(3)}$ and $x^{(5)}$ are assigned to centroid $j = 2$, then you should update this centroid as $\\mu_2 = \\frac{1}{2} (x^{(3)} + x^{(5)})$.\n",
    "\n",
    "You should now complete the function `computeCentroids(X, idx, K)` in the following code. You can implement this function using a loop over the centroids. You can also use a loop over the examples; but if you can use a vectorized implementation that does not use such a loop, your code may run faster.\n",
    "\n",
    "Once you have completed the function `computeCentroids(X, idx, K)`, you can test it by calling it once with `K = 3` centroids on the previous example dataset `X` with `idx = np.array([1, 2, 0, 0])`. If your implementation is correct, the function should return the following 3 centroids as a result:\n",
    "```python\n",
    "[[ 7.   8.5]\n",
    " [ 1.   2. ]\n",
    " [ 3.   4. ]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.  8.5]\n",
      " [1.  2. ]\n",
      " [3.  4. ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TODO:\n",
    "Complete the definition of the function computeCentroids(X, idx, K). This function \n",
    "takes as arguments the dataset X, the array of assignments idx (that indicates for \n",
    "each data-point, the index of its nearest centroid), and the number of centroids K. \n",
    "It should return a new array of centroids.\n",
    "\"\"\"\n",
    "def computeCentroids(X, idx, K):\n",
    "    new_centroids = [] # This will contain the new re-computed centroids\n",
    "    \n",
    "    # For each centroid (or cluster) index j\n",
    "    for j in range(K):\n",
    "        # TODO: find Cj, the array of all data-points that were assigned to centroid j\n",
    "        Cj = X[idx==j]\n",
    "        # TODO: re-compute the new centroid j as the mean (center) of the data-points in Cj\n",
    "        mu_j = [np.sum(Cj[:,0]),np.sum(Cj[:,1])]*1/np.abs(len(Cj))\n",
    "        # TODO: append your re-computed centroid mu_j to the list new_centroids\n",
    "        new_centroids.append(mu_j)\n",
    "    # Return new_centroids as an array\n",
    "    return np.array(new_centroids)\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Test your function computeCentroids(X, idx, K) by calling it with K = 3 on\n",
    "the previous example dataset X with idx = np.array([1, 2, 0, 0])\n",
    "\"\"\"\n",
    "idx = np.array([1, 2, 0, 0])\n",
    "centroids = computeCentroids(X, idx, K = 3)\n",
    "print(centroids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-means on an example dataset\n",
    "After you have completed the two functions (`findClosestCentroids(..)` and `computeCentroids(..)`) successfully, the next step is to use them in the main K-means algorithm on a toy 2-dimensional dataset to help you understand how K-means works. Run the following code to load the dataset and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "mat = loadmat(\"datasets/lab7data1.mat\")\n",
    "X = mat[\"X\"]\n",
    "np.random.shuffle(X)\n",
    "print(\"X.shape:\", X.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], marker=\".\", color=\"blue\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following Python code a function `Kmeans(X, K, max_iterations)` is defined. This function performs K-means clustering by calling the two functions that you implemented (`findClosestCentroids(..)` and `computeCentroids(..)`) inside a loop. It returns the final cluster centroids.\n",
    "\n",
    "Read the `Kmeans(X, K, max_iterations)` function to understand it, then call it with `K = 3` and `max_iterations = 50`, on the dataset `X` that we loaded previously. Once K-means finishes running and returns the final centroids, your task is to produce a plot of the dataset with colors corresponding to the clusters that K-means found. Your plot should be similar to the following figure.\n",
    "<img src=\"imgs/clustResultLab7A.png\" width=\"500px\" />\n",
    "**Hint:** After calling `Kmeans(..)` and getting the final centroids, you can get the cluster index to which each data-point belongs, by calling `idx = findClosestCentroids(X, centroids)` once again. Then, in order, for example, to select the data-points that are members of cluster 0, you can use `X[idx == 0]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This function performs K-means clustering and returns the final cluster centroids\n",
    "def Kmeans(X, K, max_iterations):\n",
    "    # Initialize centroids: we pick randomly K different points as our initial centroids\n",
    "    random_ids = np.random.choice(len(X), K, replace=False) # pick K random ids from range(len(X))\n",
    "    centroids = X[random_ids]\n",
    "\n",
    "    for itr in range(1, max_iterations):\n",
    "        idx = findClosestCentroids(X, centroids) # Assigning data-points to clusters\n",
    "        centroids = computeCentroids(X, idx, K)  # Updating (re-computing) the centroids\n",
    "        \n",
    "    return centroids\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Call the Kmeans(X, K, max_iterations) function with K=3 and produce a plot similar \n",
    "to the above figure. Data-points within the same cluster should have the same color.\n",
    "\"\"\"\n",
    "K = 3                # Number of clusters (and centroids) that we want to get\n",
    "max_iterations = 50  # Number of iterations to perform\n",
    "centroids = Kmeans(X, K, max_iterations)\n",
    "\n",
    "# TODO: continue here to produce the required plot\n",
    "fig, ax = plt.figure()\n",
    "idx = findClosestCentroids(X, centroids)\n",
    "X1 = X[idx==0]\n",
    "ax.scatter(X1[0], X1[1], marker = 'o', color = 'p')\n",
    "X2 = X[idx==1]\n",
    "ax.scatter(X2[0], X2[1], marker = '<', color = 'g')\n",
    "X3 = X[idx==2]\n",
    "ax.scatter(X3[0], X3[1], marker = '*', color = 'c')\n",
    "plt.show()\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image compression with K-means\n",
    "<img src=\"datasets/bird_small.png\" />\n",
    "\n",
    "In this section, you will apply K-means to image compression. In a straightforward 24-bit color representation of an image, each pixel is represented as three 8-bit unsigned integers (ranging from 0 to 255) that specify the red, green and blue intensity values. This encoding is often refered to as the RGB encoding. Our image contains thousands of colors, and in this part of the Lab, you will reduce the number of colors to 16 colors.\n",
    "\n",
    "By making this reduction, it is possible to represent (compress) the photo in an effcient way. Specifically, you only need to store the RGB values of the 16 selected colors, and for each pixel in the image you now need to only store the index of the color at that location (where only 4 bits are necessary to represent 16 possibilities).\n",
    "\n",
    "In this section, you will use the K-means algorithm to select the 16 colors that will be used to represent the compressed image. Concretely, you will treat every pixel in the original image as a data example and use the K-means algorithm to find the 16 colors that best group (cluster) the pixels in the 3-dimensional RGB space. Once you have computed the cluster centroids on the image, you will then use the 16 colors to replace the pixels in the original image.\n",
    "\n",
    "### 3.1. K-means on pixels\n",
    "In Python, images can be read as follows `A = plt.imread('image.png')`. For RGB images, this function creates a three-dimensional matrix $A$ of shape `(p, q, 3)` where $p \\times q$ is the number of pixels in the image, and each element `A[i][j]` (corresponding to the pixel at row $i$ and column $j$) is a 3-dimensional vector containing the RGB (*red*, *green*, *blue*) intensities.\n",
    "\n",
    "We consider each pixel as a 3-dimensional data-point. Therefore, the number of data-points we have is the number of pixels in the image (i.e. $n = p \\times q$). For example, for an RGB image of $128 \\times 128 = 16384 = n$ pixels, our dataset is $X \\in \\mathbb{R}^{16384 \\times 3}$.\n",
    "\n",
    "The following Python code first loads the image, and then reshapes it to create an $n \\times 3$ matrix of pixels (where $n = 16384 = 128 \\times 128$), and calls the `Kmeans(..)` function on it to cluster the pixel colors into 16 clusters. This clustering process may take some time (few seconds or minutes) as we have 16384 data-points (pixels). After finding the top K = 16 colors to represent the image, we can now assign each pixel to its closest centroid using the `findClosestCentroids(..)` function. This allows us to represent the original image using the centroid assignments of each pixel and plot the new image. The image that you will get is shown in the following figure.\n",
    "\n",
    "<img src=\"imgs/KmeansCompressedBird.png\" width=\"350px\" />\n",
    "\n",
    "Notice that you have significantly reduced the number of bits that are required to describe the image. The original image required 24 bits for each one of the $128 \\times 128$ pixel locations, while the new representation requires only requires 4 bits per pixel location. The final number of bits used would correspond to compressing the original image by about a factor of 6 (i.e. we eliminated $\\sim 83\\%$ of the original size).\n",
    "\n",
    "**Your Task:** Once you read the code and run it and see the results, you can then try to run it with a smaller value of $K$ (e.g. $k = 10, K = 5, K = 2$) and see the results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Loading the image bird_small.png into a p*q*d matrix A (here p=q=128, and d=3)\n",
    "A = plt.imread('datasets/bird_small.png')\n",
    "print(\"A.shape:\", A.shape)\n",
    "\n",
    "# Reshape A into an n*d matrix X (our dataset of n pixels/data-points)\n",
    "X = A.reshape(A.shape[0] * A.shape[1], A.shape[2])\n",
    "print(\"X.shape:\", X.shape)\n",
    "\n",
    "# Apply K-means to X to cluster the pixels into 16 clusters based on their RGB intensities.\n",
    "print(\"Performing Kmeans ... This may take some time ...\")\n",
    "centroids = Kmeans(X, K=16, max_iterations=20)\n",
    "\n",
    "# For each pixel (X[i]) in X, we find its closest centroid (idx[i])\n",
    "idx = findClosestCentroids(X, centroids)\n",
    "\n",
    "# Create a new matrix XX where each data-point is replaced by its closest centroid\n",
    "XX = np.array([ centroids[i] for i in idx ])\n",
    "\n",
    "# Reshape XX back into an image (matrix AA of dimension p*q*d)\n",
    "AA = XX.reshape(128, 128, 3)\n",
    "\n",
    "# Plot the original image A and the new image AA\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(A)\n",
    "ax1.axis(\"off\")\n",
    "ax1.set_title(\"Original image\")\n",
    "\n",
    "ax2.imshow(AA)\n",
    "ax2.axis(\"off\")\n",
    "ax2.set_title(\"Compressed image\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optional: Use your own image\n",
    "This section is optional. In this section, you can reuse the code we have supplied above to run on one of your own images. Note that if your image is very large, then K-means can take a long time to run. Therefore, we recommend that you resize your images to managable sizes (e.g. $128 \\times 128$ pixels) before running the code. You can also try to vary K to see the effects on the compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "# ...\n",
    "# ...\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
